Beginning with the standard gradient descent algorithm I implemented in a prior assignment, this project:

1. Extended GD algorithms to stochastic gradient descent (SGD).
https://www.cs.cornell.edu/courses/cs4787/2019sp/notes/lecture5.pdf 

2. Extended SGD to implement momentum.
https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d

3. Modieifed SGD to implement the Adam optimizer algorithm.
https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c`